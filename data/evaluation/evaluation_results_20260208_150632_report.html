<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>RAG System Evaluation Report</title>
    <style>
        body {
            font-family: 'Segoe UI', Tahoma, Geneva, Verdana, sans-serif;
            max-width: 1200px;
            margin: 0 auto;
            padding: 20px;
            background-color: #f5f5f5;
        }
        .header {
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            color: white;
            padding: 30px;
            border-radius: 10px;
            margin-bottom: 30px;
            box-shadow: 0 4px 6px rgba(0,0,0,0.1);
        }
        .header h1 {
            margin: 0 0 10px 0;
            font-size: 2.5em;
        }
        .header p {
            margin: 5px 0;
            opacity: 0.9;
        }
        .section {
            background: white;
            padding: 25px;
            margin-bottom: 20px;
            border-radius: 8px;
            box-shadow: 0 2px 4px rgba(0,0,0,0.1);
        }
        .section h2 {
            color: #667eea;
            border-bottom: 2px solid #667eea;
            padding-bottom: 10px;
            margin-top: 0;
        }
        .metrics-grid {
            display: grid;
            grid-template-columns: repeat(auto-fit, minmax(200px, 1fr));
            gap: 15px;
            margin: 20px 0;
        }
        .metric-card {
            background: linear-gradient(135deg, #f5f7fa 0%, #c3cfe2 100%);
            padding: 15px;
            border-radius: 8px;
            text-align: center;
        }
        .metric-card .label {
            font-size: 0.9em;
            color: #666;
            margin-bottom: 5px;
        }
        .metric-card .value {
            font-size: 2em;
            font-weight: bold;
            color: #333;
        }
        .answer-metrics {
            background: linear-gradient(135deg, #ffecd2 0%, #fcb69f 100%);
        }
        table {
            width: 100%;
            border-collapse: collapse;
            margin: 20px 0;
        }
        th, td {
            padding: 12px;
            text-align: left;
            border-bottom: 1px solid #ddd;
        }
        th {
            background-color: #667eea;
            color: white;
            font-weight: 600;
        }
        tr:hover {
            background-color: #f5f5f5;
        }
        .good { color: #27ae60; font-weight: bold; }
        .medium { color: #f39c12; font-weight: bold; }
        .poor { color: #e74c3c; font-weight: bold; }
        .info-box {
            background-color: #e3f2fd;
            border-left: 4px solid #2196f3;
            padding: 15px;
            margin: 15px 0;
            border-radius: 4px;
        }
        .footer {
            text-align: center;
            color: #666;
            margin-top: 30px;
            padding: 20px;
            border-top: 2px solid #ddd;
        }
    </style>
</head>
<body>
    <div class="header">
        <h1>ðŸŽ¯ RAG System Evaluation Report</h1>
        <p><strong>Generated:</strong> 2026-02-08 15:06:32</p>
        <p><strong>Dataset:</strong> qa_dataset_20260208_133046.json</p>
        <p><strong>Total Questions:</strong> 83</p>
        <p><strong>Evaluation Mode:</strong> Full Evaluation (Retrieval + Answer Quality)</p>
    </div>

    <div class="section">
        <h2>ðŸ“Š Overall Performance Metrics</h2>

        <div class="metrics-grid">
            <div class="metric-card">
                <div class="label">Mean Reciprocal Rank</div>
                <div class="value medium">0.6370</div>
            </div>
        </div>

        <h3>Answer Quality Metrics</h3>
        <div class="metrics-grid">

            <div class="metric-card answer-metrics">
                <div class="label">Exact Match (EM)</div>
                <div class="value poor">0.0000</div>
            </div>

            <div class="metric-card answer-metrics">
                <div class="label">F1 Score</div>
                <div class="value poor">0.0283</div>
            </div>

            <div class="metric-card answer-metrics">
                <div class="label">BLEU Score</div>
                <div class="value poor">0.0037</div>
            </div>

            <div class="metric-card answer-metrics">
                <div class="label">ROUGE-L</div>
                <div class="value poor">0.0283</div>
            </div>

            <div class="metric-card answer-metrics">
                <div class="label">Semantic Similarity</div>
                <div class="value poor">0.3700</div>
            </div>

        </div>

        <h3>Retrieval Performance</h3>
        <table>
            <tr>
                <th>Metric</th>
                <th>@3</th>
                <th>@5</th>
                <th>@10</th>
            </tr>
            <tr>
                <td><strong>Precision</strong></td>
<td class="poor">0.4980</td><td class="poor">0.4265</td><td class="poor">0.3048</td>
            </tr>
            <tr>
                <td><strong>Recall</strong></td>
<td class="medium">0.6265</td><td class="medium">0.6627</td><td class="medium">0.6627</td>
            </tr>
            <tr>
                <td><strong>Hit Rate</strong></td>
<td class="medium">0.6265</td><td class="medium">0.6627</td><td class="medium">0.6627</td>
            </tr>
        </table>

    </div>

    <div class="section">
        <h2>ðŸ“‹ Performance by Question Type</h2>

        <h3>FACTUAL (24 questions)</h3>
        <div class="metrics-grid">

            <div class="metric-card">
                <div class="label">MRR</div>
                <div class="value medium">0.5938</div>
            </div>

            <div class="metric-card answer-metrics">
                <div class="label">EM</div>
                <div class="value poor">0.0000</div>
            </div>

            <div class="metric-card answer-metrics">
                <div class="label">F1</div>
                <div class="value poor">0.0283</div>
            </div>

            <div class="metric-card answer-metrics">
                <div class="label">BLEU</div>
                <div class="value poor">0.0037</div>
            </div>

            <div class="metric-card answer-metrics">
                <div class="label">ROUGE-L</div>
                <div class="value poor">0.0283</div>
            </div>

            <div class="metric-card answer-metrics">
                <div class="label">Semantic</div>
                <div class="value poor">0.3700</div>
            </div>

        </div>

        <h3>COMPARATIVE (24 questions)</h3>
        <div class="metrics-grid">

            <div class="metric-card">
                <div class="label">MRR</div>
                <div class="value medium">0.5052</div>
            </div>

        </div>

        <h3>INFERENTIAL (24 questions)</h3>
        <div class="metrics-grid">

            <div class="metric-card">
                <div class="label">MRR</div>
                <div class="value good">0.7292</div>
            </div>

        </div>

        <h3>MULTI-HOP (11 questions)</h3>
        <div class="metrics-grid">

            <div class="metric-card">
                <div class="label">MRR</div>
                <div class="value good">0.8182</div>
            </div>

        </div>

    </div>

    <div class="section">
        <h2>ðŸ“– Metrics Interpretation Guide</h2>
        <div class="info-box">
            <h4>Answer Quality Metrics:</h4>
            <ul>
                <li><strong>Exact Match (EM):</strong> Percentage of predictions that match ground truth exactly (after normalization)</li>
                <li><strong>F1 Score:</strong> Harmonic mean of precision and recall at token level</li>
                <li><strong>BLEU:</strong> Measures n-gram overlap between prediction and reference</li>
                <li><strong>ROUGE-L:</strong> Longest common subsequence based metric</li>
                <li><strong>Semantic Similarity:</strong> Cosine similarity between answer embeddings</li>
            </ul>
        </div>

        <div class="info-box">
            <h4>Retrieval Metrics:</h4>
            <ul>
                <li><strong>MRR (Mean Reciprocal Rank):</strong> Average of reciprocal ranks of first relevant result</li>
                <li><strong>Precision@K:</strong> Proportion of relevant documents in top K results</li>
                <li><strong>Recall@K:</strong> Proportion of all relevant documents found in top K</li>
                <li><strong>Hit Rate@K:</strong> Percentage of queries with at least one relevant result in top K</li>
            </ul>
        </div>

        <div class="info-box">
            <h4>Score Interpretation:</h4>
            <ul>
                <li><span class="good">â‰¥ 0.70:</span> Good performance</li>
                <li><span class="medium">0.50 - 0.69:</span> Moderate performance</li>
                <li><span class="poor">< 0.50:</span> Needs improvement</li>
            </ul>
        </div>
    </div>

    <div class="footer">
        <p>Generated by RAG Evaluation System</p>
        <p>Report saved to: ./data/evaluation/evaluation_results_20260208_150632_report.html</p>
    </div>
</body>
</html>
